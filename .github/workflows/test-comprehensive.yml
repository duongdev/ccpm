name: Comprehensive Test Suite

on:
  push:
    branches:
      - main
      - develop
    paths:
      - 'commands/**'
      - 'hooks/**'
      - 'agents/**'
      - 'skills/**'
      - 'scripts/**'
      - 'tests/**'
      - '.claude-plugin/**'
  pull_request:
    branches:
      - main
      - develop
  workflow_dispatch:

jobs:
  # Job 1: Unit Tests (Fast, always run)
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          # Note: No npm cache - this project uses shell scripts, not npm packages

      - name: Install dependencies
        run: |
          npm install -g jq
          sudo apt-get update
          sudo apt-get install -y bc

      - name: Run unit tests
        run: |
          # TODO: Implement unit test runner
          echo "Unit tests placeholder"
          # ./tests/run-unit-tests.sh

      - name: Upload unit test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results
          path: tests/results/unit/

  # Job 2: Plugin Validation (Structural tests)
  plugin-validation:
    name: Plugin Validation
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Validate plugin structure
        run: ./scripts/validate-plugin.sh --verbose

      - name: Test skill activation
        run: ./scripts/test-skill-activation.sh --verbose

      - name: Verify hook integrity
        run: ./scripts/verify-hook-integrity.sh --verbose

      - name: Upload validation results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: plugin-validation-results
          path: scripts/*.log

  # Job 3: Mock Integration Tests (Fast, no real APIs)
  mock-integration-tests:
    name: Mock Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: |
          npm install -g jq
          cd tests/mocks/mcp-servers
          npm install

      - name: Start mock servers
        run: |
          node tests/mocks/mcp-servers/linear-mock.js --port 3001 &
          echo $! > /tmp/linear-mock.pid
          sleep 2
          # TODO: Start other mock servers when implemented
          # node tests/mocks/mcp-servers/jira-mock.js --port 3002 &
          # node tests/mocks/mcp-servers/github-mock.js --port 3003 &

      - name: Run mock integration tests
        run: |
          # TODO: Implement mock integration test runner
          echo "Mock integration tests placeholder"
          # ./tests/run-mock-tests.sh

      - name: Stop mock servers
        if: always()
        run: |
          if [ -f /tmp/linear-mock.pid ]; then
            kill $(cat /tmp/linear-mock.pid) || true
          fi

      - name: Upload mock test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: mock-integration-results
          path: tests/results/mock/

  # Job 4: Performance Benchmarks
  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 20
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y bc jq

      - name: Run token usage benchmarks
        run: |
          chmod +x tests/benchmarks/scripts/measure-token-usage.sh
          ./tests/benchmarks/scripts/measure-token-usage.sh --all --optimized

      - name: Compare with baseline
        run: |
          # TODO: Implement comparison logic
          echo "Benchmark comparison placeholder"
          # ./tests/benchmarks/scripts/compare-benchmarks.sh

      - name: Generate benchmark report
        run: |
          # TODO: Implement report generation
          echo "Report generation placeholder"
          # ./tests/benchmarks/scripts/generate-report.sh

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: performance-benchmarks
          path: |
            tests/benchmarks/token-usage/
            tests/benchmarks/reports/

      - name: Comment on PR with results
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        continue-on-error: true  # May fail if token lacks permissions on fork PRs
        with:
          script: |
            const fs = require('fs');
            // TODO: Read benchmark results and post to PR
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '## Performance Benchmark Results\n\nBenchmarks completed. See artifacts for details.'
            });

  # Job 5: Real Integration Tests (Only on main branch, uses real APIs)
  real-integration-tests:
    name: Real Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: |
          npm install -g jq
          sudo apt-get update
          sudo apt-get install -y bc

      - name: Configure test environment
        env:
          LINEAR_API_KEY: ${{ secrets.LINEAR_TEST_API_KEY }}
          LINEAR_TEST_TEAM_ID: ${{ secrets.LINEAR_TEST_TEAM_ID }}
          GITHUB_TEST_TOKEN: ${{ secrets.GITHUB_TEST_TOKEN }}
        run: |
          # Configure Linear test workspace
          echo "Configuring test environment..."
          # TODO: Implement environment setup
          # ./tests/configure-test-env.sh

      - name: Run Linear integration tests
        env:
          LINEAR_TEST_TEAM_ID: ${{ secrets.LINEAR_TEST_TEAM_ID }}
          LINEAR_TEST_CLEANUP: "true"
        run: |
          cd tests/integration
          ./run-linear-helpers-tests.sh --verbose --cleanup

      - name: Run command integration tests
        run: |
          # TODO: Implement command integration tests
          echo "Command integration tests placeholder"
          # ./tests/run-integration-tests.sh

      - name: Cleanup test data
        if: always()
        run: |
          cd tests/integration
          ./cleanup-linear-test-data.sh --all
          # TODO: Cleanup other systems
          # ./cleanup-all-test-data.sh

      - name: Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: tests/results/integration/

  # Job 6: Test Summary
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs:
      - unit-tests
      - plugin-validation
      - mock-integration-tests
      - performance-benchmarks
    if: always()
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4

      - name: Generate test summary
        run: |
          echo "# Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Component Counts" >> $GITHUB_STEP_SUMMARY
          echo "- Commands: 65+" >> $GITHUB_STEP_SUMMARY
          echo "- Skills: 10" >> $GITHUB_STEP_SUMMARY
          echo "- Agents: 4" >> $GITHUB_STEP_SUMMARY
          echo "- Hooks: 3" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Plugin Validation: Passed" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Mock Integration: Passed" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Performance Benchmarks: Completed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "See artifacts for detailed results." >> $GITHUB_STEP_SUMMARY

      - name: Check test results
        run: |
          # Fail if any required job failed
          if [ "${{ needs.plugin-validation.result }}" != "success" ]; then
            echo "Plugin validation failed"
            exit 1
          fi
          if [ "${{ needs.mock-integration-tests.result }}" != "success" ]; then
            echo "Mock integration tests failed"
            exit 1
          fi

  # Job 7: Cache Performance Analysis (Weekly)
  cache-performance:
    name: Cache Performance Analysis
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' || github.event.schedule == '0 0 * * 0'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Analyze cache performance
        run: |
          # TODO: Implement cache performance analysis
          echo "Cache performance analysis placeholder"
          # ./tests/benchmarks/performance/cache-performance.sh

      - name: Upload cache analysis
        uses: actions/upload-artifact@v4
        with:
          name: cache-performance-analysis
          path: tests/benchmarks/reports/cache-analysis.md

# Workflow for scheduled testing (nightly)
# Uncomment to enable nightly tests
# on:
#   schedule:
#     - cron: '0 0 * * *'  # Run at midnight UTC
